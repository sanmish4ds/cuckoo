{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_wiki.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNirHFLtsnZeRGXP8MkRHl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanmish4ds/cuckoo/blob/master/chatbot_wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTh92T_UIekz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "3b2aaa66-f99a-46d2-9eef-8545eef5904e"
      },
      "source": [
        "pip install wikipedia"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.4.5.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=f7bf1813a14352e3925c9c404c088f14e29cbc9dff5477eedf6b995ba8fa9148\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C14wCFIFI0wH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "204853b8-a5dc-48dd-c380-5e8b9efe115d"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import re, string, unicodedata\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import wikipedia as wk\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt') \n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEfit8INJnf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "570d1ad7-c4be-4ec8-f11f-5fef02c0a9a5"
      },
      "source": [
        "data=open('/content/wiki_chatbot.txt',mode='r',errors=None)\n",
        "raw=data.read().lower()\n",
        "raw[:1000]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'human resource management\\n\\n\\nhuman resource management is the process of recruiting, selecting, inducting employees, providing orientation, imparting training and development, appraising the performance of employees, deciding compensation and providing benefits, motivating employees, maintaining proper relations with employees and their trade unions, ensuring employees safety, welfare and healthy measures in compliance with labour laws of the land and finally following the orders / judgements of the concern high court and supreme court, if any.\\n\\n\\n\\n\\n\\nhuman resource management involves management functions like planning, organizing, directing and controlling\\nit involves procurement, development, maintenance of human resource\\nit helps to achieve individual, organizational and social objectives\\nhuman resource management is a multidisciplinary subject. it includes the study of management, psychology, communication, economics and sociology.\\nit involves team spirit and team work.\\nit is a conti'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56euvTJuLGLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)\n",
        "sent_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_t31xfkLRtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = nltk.word_tokenize(raw)\n",
        "word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtHusPzwLvgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "  remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "  word = nltk.word_tokenize(raw.translate(remove_punct_dict))\n",
        "  #remove ascii\n",
        "  new_words = []\n",
        "  for word in word_token:\n",
        "      new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "      new_words.append(new_word)\n",
        "  \n",
        "  #Remove tags\n",
        "  rmv = []\n",
        "  for w in new_words:\n",
        "      text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
        "      rmv.append(text)\n",
        "  #pos tagging and lemmatization\n",
        "  tag_map = defaultdict(lambda : wn.NOUN)\n",
        "  tag_map['J'] = wn.ADJ\n",
        "  tag_map['V'] = wn.VERB\n",
        "  tag_map['R'] = wn.ADV\n",
        "  lmtzr = WordNetLemmatizer()\n",
        "  lemma_list = []\n",
        "  rmv = [i for i in rmv if i]\n",
        "  for token, tag in nltk.pos_tag(rmv):\n",
        "      lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "      lemma_list.append(lemma)\n",
        "  return lemma_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRSNhMoUOh9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "welcome_input = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "welcome_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def welcome(user_response):\n",
        "    for word in user_response.split():\n",
        "        if word.lower() in welcome_input:\n",
        "            return random.choice(welcome_response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ-8X-xxPPN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateResponse(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    vals = linear_kernel(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
        "        print(\"Checking Wikipedia\")\n",
        "        if user_response:\n",
        "            robo_response = wikipedia_data(user_response)\n",
        "            return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "#wikipedia search\n",
        "def wikipedia_data(input):\n",
        "    reg_ex = re.search('tell me about (.*)', input)\n",
        "    try:\n",
        "        if reg_ex:\n",
        "            topic = reg_ex.group(1)\n",
        "            wiki = wk.summary(topic, sentences = 3)\n",
        "            return wiki\n",
        "    except Exception as e:\n",
        "            print(\"No content has been found\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY2DBdSzPh0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "e4a861b8-00d3-4d9e-f46a-8e5865eff994"
      },
      "source": [
        "flag=True\n",
        "print(\"My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"Chatterbot : You are welcome..\")\n",
        "        else:\n",
        "            if(welcome(user_response)!=None):\n",
        "                print(\"Chatterbot : \"+welcome(user_response))\n",
        "            else:\n",
        "                print(\"Chatterbot : \",end=\"\")\n",
        "                print(generateResponse(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"Chatterbot : Bye!!! \")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\n",
            "tell me\n",
            "Chatterbot : tell me about cricket\n",
            "tell me about cricket\n",
            "Chatterbot : Checking Wikipedia\n",
            "Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 20-metre (22-yard) pitch with a wicket at each end, each comprising two bails balanced on three stumps. The batting side scores runs by striking the ball bowled at the wicket with the bat, while the bowling and fielding side tries to prevent this and dismiss each player (so they are \"out\"). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side catching the ball after it is hit by the bat, but before it hits the ground.\n",
            "tell me about bihar\n",
            "Chatterbot : Checking Wikipedia\n",
            "Bihar (; Hindi pronunciation: [bɪˈɦaːr] (listen)) is a state in eastern India. It is the third-largest state by population and twelfth-largest by territory, with an area of 94,163 km2 (36,357 sq mi). It is contiguous with Uttar Pradesh to its west, Nepal to the north, the northern part of West Bengal to the east, and with Jharkhand to the south.\n",
            "what is corona\n",
            "Chatterbot : Checking Wikipedia\n",
            "None\n",
            "tell me about corona\n",
            "Chatterbot : Checking Wikipedia\n",
            "No content has been found\n",
            "None\n",
            "tell me about covid19\n",
            "Chatterbot : Checking Wikipedia\n",
            "COVID-19 testing can identify the SARS-CoV-2 virus and includes methods that detect the presence of the virus itself (RT-PCR, isothermal nucleic acid amplification, antigen) and those that detect antibodies produced in response to infection. Detection of antibodies (serological tests) can be used both for diagnosis and for population surveillance. Antibody tests show how many people have had the disease, including those whose symptoms were minor or who were asymptomatic, but may not find antibodies in someone with a current COVID-19 infection since antibodies may not show up for weeks.\n",
            "what is data science\n",
            "Chatterbot : data warehousing: knowing how to process and store hr data efficiently, automation of  collection of data and cleaning data.\n",
            "machine learning\n",
            "Chatterbot : it ensures continuous skill development of employees working in organisation and habituates process of learning for developing knowledge to work.\n",
            "bye\n",
            "Chatterbot : Bye!!! \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}